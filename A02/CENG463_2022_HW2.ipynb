{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name: Joel Amarou Heuer\n",
    "\n",
    "Student ID: 202102201"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/korosu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math     \n",
    "\n",
    "# for tokenization use nltk\n",
    "import time\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Hyper)Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Hyperparameters\n",
    "# input_size = \n",
    "output_size = 10 # -> number of neurons in output-layer (= for each possible rating?)\n",
    "# [hidden_layers_sizes] = \n",
    "# learning_rate = \n",
    "number_of_epochs = 1 # how is every sample used for pass/forward/backward\n",
    "\n",
    "# Parameters\n",
    "train_data_path = \"./data/drugLibTrain_raw.tsv\" # please use relative path like this\n",
    "test_data_path = \"./data/drugLibTest_raw.tsv\" # please use relative path like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "EXAMPLE\n",
    "\n",
    "inputs = [1,2,3,4]    das sind 4 Eingaben, also entweder \n",
    "\t\t\t\t\t\t-> 4 Features fÃ¼r Input-Layer oder \n",
    "\t\t\t\t\t\t-> 4 Neuronen der davorigen Layer\n",
    "\n",
    "weights = [\n",
    "\t\t\t[w11,w12,w13,w14]       -> Neuoron1 hat 4 Weights, also von jedem Neuron der davorigen Schicht eins\n",
    "\t\t\t[w21,w22,w23,w24]\t\t-> Neuoron2 hat 4 Weights, also von jedem Neuron der davorigen Schicht eins\n",
    "\t\t]\n",
    "\t  2x4\n",
    "\t#NeuronenProHiddenLayer x #Inputs\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def forward_pass(data):\n",
    "\t'''\n",
    "\t@param data is comment/review in string format\n",
    "\t-> for a string s, one-hot-encoding makes matrix with dimensions:     {#words-in-string}         x     {#all-words-in-dataset = input-size}\n",
    "\n",
    "\tmultiply with weights-matrix, which has dimensions:\t\t\t\t:     {#neuron-in-hidden-layeri} x     {#input-size}\n",
    "\n",
    "\n",
    "\tFormula1: \tweights     *      inputs       +     biases\n",
    "\t       \t    {#n/l}x{#ip}       {#ip}{??}          {#ip}{??}\n",
    "\n",
    "\n",
    "\t\n",
    "\n",
    "\t\n",
    "\t'''\n",
    "\n",
    "\tprint(data)\n",
    "\t# LayerInput --> LayerHidden1\n",
    "\n",
    "\n",
    "\t# LayerHidden2 --> LayerHidden2\n",
    "\n",
    "\t# LayerHidden2 --> LayerOutput\n",
    "\tpass\n",
    "\n",
    "\n",
    "def train(train_data, train_labels, valid_data, valid_labels):\n",
    "\n",
    "\tfor epoch in range(number_of_epochs):\n",
    "\t\tindex = 0\n",
    "\n",
    "#\t\t# Same thing about [hidden_layers] mentioned above is valid here also\n",
    "#\t\tfor data, labels in zip(train_data, train_labels):\n",
    "#\t\t\tpredictions, [hidden_layers] = forward_pass(data)\n",
    "#\t\t\tloss_signals = derivation_of_loss_function(labels, predictions)\n",
    "#\t\t\tbackward_pass(data, [hidden_layers], predictions, loss_signals)\n",
    "#\t\t\tloss = loss_function(labels, predictions)\n",
    "#\n",
    "#\t\t\tif index % 400 == 0: # at each 400th sample, we run validation set to see our model's improvements\n",
    "#\t\t\t\taccuracy, loss = test(valid_data, valid_labels)\n",
    "#\t\t\t\tprint(\"Epoch= \"+str(epoch)+\", Coverage= %\"+ str(100*(index/len(train_data))) + \", Accuracy= \"+ str(accuracy) + \", Loss= \" + str(loss))\n",
    "#\n",
    "#\t\t\tindex += 1\n",
    "#\n",
    "#\treturn losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features =  'commentsReview'\n",
    "\n",
    "###########################################\n",
    "################ Load Data ################\n",
    "###########################################\n",
    "# load data\n",
    "train_data = pd.read_csv(train_data_path, sep='\\t')\n",
    "test_data = pd.read_csv(test_data_path, sep='\\t')\n",
    "# drop nans\n",
    "train_data.dropna(subset = [input_features], inplace=True)\n",
    "test_data.dropna(subset = [input_features], inplace=True)\n",
    "# lowercase all comments\n",
    "train_data[input_features] = train_data[input_features].str.lower()\n",
    "test_data[input_features] = test_data[input_features].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words 11584\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "################ Create directory for words ################\n",
    "############################################################\n",
    "# merge data\n",
    "df_data =  pd.concat([train_data,test_data], axis=0)\n",
    "# create dictonary\n",
    "dictonary_words = df_data[\"commentsReview\"].apply(nltk.word_tokenize)\n",
    "dictonary_words = dictonary_words.values.tolist()\n",
    "# flat lists\n",
    "dictonary_words = [item for sublist in dictonary_words for item in sublist]\n",
    "# drop duplicates\n",
    "dictonary_words = list(set(dictonary_words))\n",
    "# number of all words\n",
    "dict_size = len(dictonary_words)\n",
    "\n",
    "print(f\"Number of words {dict_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "################ Tokenization ################\n",
    "##############################################\n",
    "train_data_tokenized:pd.DataFrame = train_data[\"commentsReview\"].apply(nltk.word_tokenize)\n",
    "test_data_tokenized:pd.DataFrame = test_data[\"commentsReview\"].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "################ One-Hot-Encoding ################\n",
    "##################################################\n",
    "\n",
    "# tokenized sentence to onehot\n",
    "def tokenized_to_onehot(tokenized_sentence):   \n",
    "    one_hot_matrix = []\n",
    "    for word in tokenized_sentence:\n",
    "        # create one-hot array for word\n",
    "        one_hot_word = np.zeros(shape=(dict_size))\n",
    "        i_hot = dictonary_words.index(word)\n",
    "        # set the 1 at index of dictonary\n",
    "        one_hot_word[i_hot] = 1\n",
    "        one_hot_matrix.append(one_hot_word)\n",
    "    return one_hot_matrix\n",
    "\n",
    "\n",
    "def split_computation(tokenized):\n",
    "    '''\n",
    "    just a support function. \n",
    "    somehow my computer crashes when i try to compute everthing in one operation\n",
    "    '''\n",
    "    size = len(tokenized)\n",
    "    chunks = range(0,size,math.floor(size/15))\n",
    "    splits = len(chunks)\n",
    "\n",
    "    hot_list = [] \n",
    "    for i in range(0, splits):\n",
    "        print(f'{i}/{splits}')\n",
    "        time.sleep(10)\n",
    "        if i==0: # first\n",
    "            continue\n",
    "        elif i == (splits -1): # last\n",
    "            hot_list.append(tokenized.iloc[chunks[i-1]:size].apply(tokenized_to_onehot))\n",
    "        else: # normal\n",
    "            hot_list.append(tokenized.iloc[chunks[i-1]:chunks[i]].apply(tokenized_to_onehot))\n",
    "\n",
    "    return pd.concat(hot_list, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/16\n",
      "1/16\n",
      "2/16\n",
      "3/16\n",
      "4/16\n",
      "5/16\n",
      "6/16\n",
      "7/16\n",
      "8/16\n",
      "9/16\n",
      "10/16\n",
      "11/16\n",
      "12/16\n",
      "13/16\n",
      "14/16\n",
      "15/16\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "train_data_onehot = split_computation(train_data_tokenized)\n",
    "\n",
    "# merge into dataframe\n",
    "train_data[f'{input_features}_onehot'] = train_data_onehot\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/16\n",
      "1/16\n",
      "2/16\n",
      "3/16\n",
      "4/16\n",
      "5/16\n",
      "6/16\n",
      "7/16\n",
      "8/16\n",
      "9/16\n",
      "10/16\n",
      "11/16\n",
      "12/16\n",
      "13/16\n",
      "14/16\n",
      "15/16\n"
     ]
    }
   ],
   "source": [
    "test_data_onehot = split_computation(test_data_tokenized)\n",
    "\n",
    "# merge into dataframe\n",
    "test_data[f'{input_features}_onehot'] = test_data_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "147",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/study-lab/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   <a href='file:///home/korosu/miniconda3/envs/study-lab/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3619'>3620</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/korosu/miniconda3/envs/study-lab/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3620'>3621</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   <a href='file:///home/korosu/miniconda3/envs/study-lab/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3621'>3622</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/study-lab/lib/python3.9/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/study-lab/lib/python3.9/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2131\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2140\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 147",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/korosu/git/university/CENG-463-ML/A02/CENG463_2022_HW2.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/korosu/git/university/CENG-463-ML/A02/CENG463_2022_HW2.ipynb#ch0000013?line=24'>25</a>\u001b[0m \u001b[39m# add 1's into one-hot-matrix\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/korosu/git/university/CENG-463-ML/A02/CENG463_2022_HW2.ipynb#ch0000013?line=25'>26</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(train_y)) :    \n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/korosu/git/university/CENG-463-ML/A02/CENG463_2022_HW2.ipynb#ch0000013?line=26'>27</a>\u001b[0m     new_train_y[i][train_y[i]\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/korosu/git/university/CENG-463-ML/A02/CENG463_2022_HW2.ipynb#ch0000013?line=28'>29</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(test_y)):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/korosu/git/university/CENG-463-ML/A02/CENG463_2022_HW2.ipynb#ch0000013?line=29'>30</a>\u001b[0m     new_test_y[i][test_y[i]\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/study-lab/lib/python3.9/site-packages/pandas/core/series.py:958\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    <a href='file:///home/korosu/miniconda3/envs/study-lab/lib/python3.9/site-packages/pandas/core/series.py?line=954'>955</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[1;32m    <a href='file:///home/korosu/miniconda3/envs/study-lab/lib/python3.9/site-packages/pandas/core/series.py?line=956'>957</a>\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> <a href='file:///home/korosu/miniconda3/envs/study-lab/lib/python3.9/site-packages/pandas/core/series.py?line=957'>958</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[1;32m    <a href='file:///home/korosu/miniconda3/envs/study-lab/lib/python3.9/site-packages/pandas/core/series.py?line=959'>960</a>\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    <a href='file:///home/korosu/miniconda3/envs/study-lab/lib/python3.9/site-packages/pandas/core/series.py?line=960'>961</a>\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/korosu/miniconda3/envs/study-lab/lib/python3.9/site-packages/pandas/core/series.py?line=961'>962</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/korosu/miniconda3/envs/study-lab/lib/python3.9/site-packages/pandas/core/series.py?line=962'>963</a>\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/study-lab/lib/python3.9/site-packages/pandas/core/series.py:1069\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   <a href='file:///home/korosu/miniconda3/envs/study-lab/lib/python3.9/site-packages/pandas/core/series.py?line=1065'>1066</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[1;32m   <a href='file:///home/korosu/miniconda3/envs/study-lab/lib/python3.9/site-packages/pandas/core/series.py?line=1067'>1068</a>\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/korosu/miniconda3/envs/study-lab/lib/python3.9/site-packages/pandas/core/series.py?line=1068'>1069</a>\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[1;32m   <a href='file:///home/korosu/miniconda3/envs/study-lab/lib/python3.9/site-packages/pandas/core/series.py?line=1069'>1070</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n",
      "File \u001b[0;32m~/miniconda3/envs/study-lab/lib/python3.9/site-packages/pandas/core/indexes/base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   <a href='file:///home/korosu/miniconda3/envs/study-lab/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3620'>3621</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   <a href='file:///home/korosu/miniconda3/envs/study-lab/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3621'>3622</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> <a href='file:///home/korosu/miniconda3/envs/study-lab/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3622'>3623</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/korosu/miniconda3/envs/study-lab/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3623'>3624</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/korosu/miniconda3/envs/study-lab/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3624'>3625</a>\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/korosu/miniconda3/envs/study-lab/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3625'>3626</a>\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/korosu/miniconda3/envs/study-lab/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3626'>3627</a>\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/korosu/miniconda3/envs/study-lab/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3627'>3628</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 147"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "################ Create datasets ################\n",
    "#################################################\n",
    "# split into train & test    and    feature & y columns\n",
    "train_x = train_data[input_features]\n",
    "train_y = train_data['rating']\n",
    "test_x = test_data[input_features]\n",
    "test_y = test_data['rating']\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "################ # ONE-HOT for output (classes 1-10) ################\n",
    "#####################################################################\n",
    "# creating one-hot vector notation of labels. \n",
    "# (Labels are given numeric in the dataset)\n",
    "new_train_y = np.zeros(shape=(len(train_y), output_size))\n",
    "new_test_y = np.zeros(shape=(len(test_y), output_size))\n",
    "\n",
    "# add 1's into one-hot-matrix\n",
    "for i in range(len(train_y)) :    \n",
    "    new_train_y[i][train_y[i]-1] = 1\n",
    "\n",
    "for i in range(len(test_y)):\n",
    "    new_test_y[i][test_y[i]-1] = 1\n",
    "\n",
    "train_y = new_train_y\n",
    "test_y = new_test_y\n",
    "\n",
    "\n",
    "##################################################################\n",
    "################ split into validation & training ################\n",
    "##################################################################\n",
    "# Split training data into [a] Training (75%) and [b] validation (25%)\n",
    "valid_x = np.asarray(train_x[int(0.75*len(train_x)):-1])\n",
    "valid_y = np.asarray(train_y[int(0.75*len(train_y)):-1])\n",
    "train_x = np.asarray(train_x[0:int(0.75*len(train_x))])\n",
    "train_y = np.asarray(train_y[0:int(0.75*len(train_y))])\n",
    "\n",
    "\n",
    "\n",
    "#######################################\n",
    "################ train ################\n",
    "#######################################\n",
    "# TODO\n",
    "train(train_x, train_y, valid_x, valid_y)\n",
    "\n",
    "######################################\n",
    "################ test ################\n",
    "######################################\n",
    "# TODO \n",
    "#print(\"Test Scores:\")\n",
    "#print(test(test_x, test_y))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f81ca3d73f16c3fc9f14852cf151c9505b747a4fab6a8a5fe026d3697dfd654b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('study-lab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
