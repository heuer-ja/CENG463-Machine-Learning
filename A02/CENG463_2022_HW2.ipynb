{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name: Joel Amarou Heuer\n",
    "\n",
    "Student ID: 202102201"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/korosu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math     \n",
    "\n",
    "# for tokenization use nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Hyper)Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Hyperparameters\n",
    "# input_size = \n",
    "output_size = 10 # -> number of neurons in output-layer (= for each possible rating?)\n",
    "# [hidden_layers_sizes] = \n",
    "# learning_rate = \n",
    "number_of_epochs = 1 # how is every sample used for pass/forward/backward\n",
    "\n",
    "\n",
    "# Parameters\n",
    "train_data_path = \"./data/drugLibTrain_raw.tsv\" # please use relative path like this\n",
    "test_data_path = \"./data/drugLibTest_raw.tsv\" # please use relative path like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features =  'commentsReview'\n",
    "\n",
    "###########################################\n",
    "################ Load Data ################\n",
    "###########################################\n",
    "# load data\n",
    "train_data = pd.read_csv(train_data_path, sep='\\t')\n",
    "test_data = pd.read_csv(test_data_path, sep='\\t')\n",
    "# drop nans\n",
    "train_data.dropna(subset = [input_features], inplace=True)\n",
    "test_data.dropna(subset = [input_features], inplace=True)\n",
    "# lowercase all comments\n",
    "train_data[input_features] = train_data[input_features].str.lower()\n",
    "test_data[input_features] = test_data[input_features].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words 11584\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "################ Create directory for words ################\n",
    "############################################################\n",
    "# merge data\n",
    "df_data =  pd.concat([train_data,test_data], axis=0)\n",
    "# create dictonary\n",
    "dictonary_words = df_data[\"commentsReview\"].apply(nltk.word_tokenize)\n",
    "dictonary_words = dictonary_words.values.tolist()\n",
    "# flat lists\n",
    "dictonary_words = [item for sublist in dictonary_words for item in sublist]\n",
    "# drop duplicates\n",
    "dictonary_words = list(set(dictonary_words))\n",
    "# number of all words\n",
    "dict_size = len(dictonary_words)\n",
    "\n",
    "print(f\"Number of words {dict_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "################ Tokenization ################\n",
    "##############################################\n",
    "train_data_tokenized:pd.DataFrame = train_data[\"commentsReview\"].apply(nltk.word_tokenize)\n",
    "test_data_tokenized:pd.DataFrame = test_data[\"commentsReview\"].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>urlDrugName</th>\n",
       "      <th>rating</th>\n",
       "      <th>effectiveness</th>\n",
       "      <th>sideEffects</th>\n",
       "      <th>condition</th>\n",
       "      <th>benefitsReview</th>\n",
       "      <th>sideEffectsReview</th>\n",
       "      <th>commentsReview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1540</th>\n",
       "      <td>3248</td>\n",
       "      <td>nuvaring</td>\n",
       "      <td>3</td>\n",
       "      <td>Highly Effective</td>\n",
       "      <td>Extremely Severe Side Effects</td>\n",
       "      <td>i kept getting pregnant. no more stairs.... pl...</td>\n",
       "      <td>Never got pregnant</td>\n",
       "      <td>My dosage was so high that at the end of a wee...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>4123</td>\n",
       "      <td>chantix</td>\n",
       "      <td>7</td>\n",
       "      <td>Highly Effective</td>\n",
       "      <td>Moderate Side Effects</td>\n",
       "      <td>nicotine addiction</td>\n",
       "      <td>I had smoked for 20 years and despite repeated...</td>\n",
       "      <td>I was nauseated after taking each tablet. The ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 urlDrugName  rating     effectiveness  \\\n",
       "1540        3248    nuvaring       3  Highly Effective   \n",
       "2221        4123     chantix       7  Highly Effective   \n",
       "\n",
       "                        sideEffects  \\\n",
       "1540  Extremely Severe Side Effects   \n",
       "2221          Moderate Side Effects   \n",
       "\n",
       "                                              condition  \\\n",
       "1540  i kept getting pregnant. no more stairs.... pl...   \n",
       "2221                                 nicotine addiction   \n",
       "\n",
       "                                         benefitsReview  \\\n",
       "1540                                 Never got pregnant   \n",
       "2221  I had smoked for 20 years and despite repeated...   \n",
       "\n",
       "                                      sideEffectsReview  \\\n",
       "1540  My dosage was so high that at the end of a wee...   \n",
       "2221  I was nauseated after taking each tablet. The ...   \n",
       "\n",
       "                                         commentsReview  \n",
       "1540  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2221  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############################################################\n",
    "################ Word Encoding (Bag of words) ################\n",
    "##############################################################\n",
    "def to_bow(dataset):\n",
    "    '''\n",
    "    @param dataset which should be considered (train or test)\n",
    "    @return new column for dataset with sentence/word-encoded cells\n",
    "\n",
    "    For each cell in column of interest, following is performed:\n",
    "    (1) cell contains sentence\n",
    "    (2) sentence is tokenized --> produces array of words/tokens\n",
    "    (3) create array A with len(dict_size) \n",
    "    (4) for each token count occurance \n",
    "    (5) A[index of token t in dict] = occurance of t\n",
    "    '''\n",
    "    new_col = []\n",
    "\n",
    "    # for each row\n",
    "    for _, row in dataset.iterrows():\n",
    "        tokenized = nltk.word_tokenize(row[input_features])\n",
    "        array = np.zeros(shape=dict_size)\n",
    "\n",
    "        # for each word in a setence\n",
    "        for word in tokenized:\n",
    "            i_hot = dictonary_words.index(word)\n",
    "            array[i_hot] = array[i_hot] + 1\n",
    "        new_col.append(array)\n",
    "    \n",
    "    return new_col\n",
    "\n",
    "\n",
    "test_data[input_features] =  to_bow(test_data)\n",
    "train_data[input_features] =  to_bow(train_data)\n",
    "train_data.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.42169984  4.29525646  0.          6.24997091  7.01504583  5.12781748\n",
      "   0.          9.94769998  0.         11.26922242]]\n",
      "\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "################ Create datasets ################\n",
    "#################################################\n",
    "# split into train & test    and    feature & y columns\n",
    "train_x = train_data[input_features]\n",
    "train_y = train_data['rating']\n",
    "test_x = test_data[input_features]\n",
    "test_y = test_data['rating']\n",
    "\n",
    "#####################################################################\n",
    "################ # ONE-HOT for output (classes 1-10) ################\n",
    "#####################################################################\n",
    "# creating one-hot vector notation of labels. \n",
    "# (Labels are given numeric in the dataset)\n",
    "new_train_y = np.zeros(shape=(len(train_y), output_size))\n",
    "new_test_y = np.zeros(shape=(len(test_y), output_size))\n",
    "\n",
    "# add 1's into one-hot-matrix\n",
    "for i in range(len(test_y)):\n",
    "    new_test_y[i][test_y.iloc[i]-1] = 1\n",
    "\n",
    "for i in range(len(train_y)) :    \n",
    "    new_train_y[i][train_y.iloc[i]-1] = 1\n",
    "\n",
    "train_y = new_train_y\n",
    "test_y = new_test_y\n",
    "\n",
    "\n",
    "##################################################################\n",
    "################ split into validation & training ################\n",
    "##################################################################\n",
    "# Split training data into [a] Training (75%) and [b] validation (25%)\n",
    "valid_x = np.asarray(train_x[int(0.75*len(train_x)):-1])\n",
    "valid_y = np.asarray(train_y[int(0.75*len(train_y)):-1])\n",
    "train_x = np.asarray(train_x[0:int(0.75*len(train_x))])\n",
    "train_y = np.asarray(train_y[0:int(0.75*len(train_y))])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''WIP'''\n",
    "# Hyperparamater\n",
    "num_neurons_in_h1 = 250\n",
    "num_neurons_in_h2 = 200\n",
    "\n",
    "w_b = {\n",
    "    # Input-Layer -> H1\n",
    "    \"L0_w\" : np.random.randn(num_neurons_in_h1, dict_size), \n",
    "    \"L0_b\": np.ones((num_neurons_in_h1, 1)) * 0.01,\n",
    "\n",
    "    # H1 -> H2\n",
    "    \"L1_w\" : np.random.randn(num_neurons_in_h2, num_neurons_in_h1), # WIP\n",
    "    \"L1_b\": np.ones((num_neurons_in_h2, 1)) * 0.01,\n",
    "\n",
    "    # H2 -> Output-Layer\n",
    "    \"L2_w\" : np.random.randn(output_size, num_neurons_in_h2), # WIP\n",
    "    \"L2_b\": np.ones((output_size, 1)) * 0.01,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################################\n",
    "################ train ################\n",
    "#######################################\n",
    "# TODO\n",
    "train(train_x, train_y, valid_x, valid_y)\n",
    "\n",
    "######################################\n",
    "################ test ################\n",
    "######################################\n",
    "# TODO \n",
    "#print(\"Test Scores:\")\n",
    "#print(test(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_function(z):\n",
    "\t'''\n",
    "\tsigmoid-function is used as activation function for hidden-layers\n",
    "\t'''\n",
    "\treturn 1/(1 + np.exp(-z))\n",
    "\n",
    "def reLU(z):\n",
    "    '''\n",
    "\treLU is used as activation for output-layer\n",
    "\t'''\n",
    "    return np.maximum(0.0, z)\n",
    "\n",
    "\n",
    "def forward_pass(data):\n",
    "\t# Input --> Hidden-1\n",
    "\tz0 = np.dot(data, w_b['L0_w'].T) + w_b['L0_b'].T\n",
    "\ta0 = activation_function(z0)\n",
    "\n",
    "\t# Hidden-1 --> Hidden-2\n",
    "\tz1 = np.dot(a0, w_b['L1_w'].T) + w_b['L1_b'].T\n",
    "\ta1 = activation_function(z1)\n",
    "\n",
    "\n",
    "\t# LayerHidden2 --> LayerOutput\n",
    "\tz2 = np.dot(a1, w_b['L2_w'].T) + w_b['L2_b'].T\n",
    "\ty_pred = reLU(z2)\n",
    "\n",
    "\tresults={\n",
    "\t\t\"a0\": a0,\n",
    "\t\t\"a1\": a1,\n",
    "\t\t\"y_pred\": y_pred,\n",
    "\t}\n",
    "\treturn results\n",
    "\n",
    "def train(train_data, train_labels, valid_data, valid_labels):\n",
    "\n",
    "\tfor epoch in range(number_of_epochs):\n",
    "\t\tindex = 0\n",
    "\n",
    "\t\t# Same thing about [hidden_layers] mentioned above is valid here also\n",
    "\t\tfor data, labels in zip(train_data, train_labels):\n",
    "\t\t\tpredictions = forward_pass(data)\n",
    "\t\t\tbreak\n",
    "#\t\t\tloss_signals = derivation_of_loss_function(labels, predictions)\n",
    "#\t\t\tbackward_pass(data, [hidden_layers], predictions, loss_signals)\n",
    "#\t\t\tloss = loss_function(labels, predictions)\n",
    "#\n",
    "#\t\t\tif index % 400 == 0: # at each 400th sample, we run validation set to see our model's improvements\n",
    "#\t\t\t\taccuracy, loss = test(valid_data, valid_labels)\n",
    "#\t\t\t\tprint(\"Epoch= \"+str(epoch)+\", Coverage= %\"+ str(100*(index/len(train_data))) + \", Accuracy= \"+ str(accuracy) + \", Loss= \" + str(loss))\n",
    "#\n",
    "#\t\t\tindex += 1\n",
    "#\n",
    "#\treturn losses"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f81ca3d73f16c3fc9f14852cf151c9505b747a4fab6a8a5fe026d3697dfd654b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('study-lab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
